{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tr = pickle.load(open('Dataset/tr.pkl', 'rb'))\n",
    "vl = pickle.load(open('Dataset/vl.pkl', 'rb'))\n",
    "ts = pickle.load(open('Dataset/ts.pkl', 'rb'))\n",
    "\n",
    "print(len(tr), len(vl), len(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = 10\n",
    "LEN = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class DS(Dataset):\n",
    "    def __init__(self, dat):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dat = dat\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp, dec_inp, dec_out = self.dat[idx]\n",
    "        inp, dec_inp, dec_out = np.asarray(inp), np.asarray(dec_inp), np.asarray(dec_out)\n",
    "        wgt = 1.0-(dec_inp==(VOCAB+1))\n",
    "        \n",
    "        return inp, dec_inp, dec_out, wgt\n",
    "\n",
    "ld_tr = DataLoader(DS(tr), batch_size=50, shuffle=True)\n",
    "ld_vl = DataLoader(DS(vl), batch_size=100)\n",
    "ld_ts = DataLoader(DS(ts), batch_size=100)\n",
    "\n",
    "for inp, dec_inp, dec_out, wgt in ld_tr:\n",
    "    print(inp.shape, dec_inp.shape, dec_out.shape, wgt.shape)\n",
    "    \n",
    "    print(inp[0], dec_inp[0], dec_out[0], wgt[0])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, EMB=8, HID=64, DP=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.EMB = EMB\n",
    "        self.HID = HID\n",
    "        self.DP = nn.Dropout(DP)\n",
    "        \n",
    "        self.emb = nn.Embedding(VOCAB+2, self.EMB)\n",
    "        \n",
    "        self.enc = nn.GRU(self.EMB, self.HID, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "        self.dec = nn.GRU(self.EMB, self.HID*2, \n",
    "                          batch_first=True)\n",
    "        self.att = nn.Parameter(T.FloatTensor(self.HID*2, self.HID*2))\n",
    "        \n",
    "        self.fc = nn.Linear(self.HID*2*2, VOCAB+2)\n",
    "        \n",
    "        self.init()\n",
    "        \n",
    "    def init(self):\n",
    "        stdv = 1/math.sqrt(self.HID*2)\n",
    "        \n",
    "        self.att.data.uniform_(stdv, -stdv)\n",
    "    \n",
    "    def run_dec(self, dec_inp, out_enc, h):\n",
    "        dec_inp = self.emb(dec_inp)\n",
    "        out_dec, h = self.dec(dec_inp, h)\n",
    "        out_dec = self.DP(out_dec)\n",
    "        \n",
    "        att_wgt = nn.functional.softmax(T.bmm(T.matmul(out_dec, self.att), out_enc.transpose(1, 2)), dim=2)\n",
    "        att_cxt = T.bmm(att_wgt, out_enc)\n",
    "            \n",
    "        out = T.cat([out_dec, att_cxt], dim=2)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, h\n",
    "    \n",
    "    def forward(self, inp, \n",
    "                is_tr=False, dec_inp=None):\n",
    "        \n",
    "        inp = self.emb(inp)\n",
    "        out_enc, h = self.enc(inp)\n",
    "        out_enc = self.DP(out_enc)\n",
    "        h = h.view((1, inp.shape[0], 2*self.HID))\n",
    "        \n",
    "        if is_tr==True:\n",
    "            out, _ = self.run_dec(dec_inp, out_enc, h)\n",
    "            \n",
    "            return out\n",
    "        \n",
    "        else:\n",
    "            dec_inp = T.ones((inp.shape[0], 1)).long().cuda() * VOCAB # START\n",
    "            \n",
    "            outs = []\n",
    "            for _ in range(LEN):\n",
    "                out, h = self.run_dec(dec_inp, out_enc, h)\n",
    "                dec_inp = T.argmax(out, dim=2, keepdim=False)\n",
    "                \n",
    "                outs.append(out)\n",
    "                \n",
    "            out = T.cat(outs, dim=1)\n",
    "            \n",
    "            return out\n",
    "\n",
    "model = Model().cuda()\n",
    "print(model)\n",
    "\n",
    "out = model(inp.long().cuda(), is_tr=True, dec_inp=dec_inp.long().cuda())\n",
    "print(out.shape)\n",
    "\n",
    "out = model(inp.long().cuda())\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu as BLEU\n",
    "\n",
    "ref = [[1, 2, 3, 4, 5, 6]]\n",
    "cnd = [1, 3, 4, 5, 6]\n",
    "bleu = BLEU(ref, cnd)\n",
    "\n",
    "print('BLEU: %.4f%%'%(bleu*100))\n",
    "\n",
    "def get_bleu(out, dec_out):\n",
    "    out = out.tolist()\n",
    "    dec_out = dec_out.tolist()\n",
    "    \n",
    "    if VOCAB+1 in out:\n",
    "        cnd = out[:out.index(VOCAB+1)]\n",
    "    else:\n",
    "        cnd = out\n",
    "    \n",
    "    if VOCAB+1 in dec_out:\n",
    "        ref = [dec_out[:dec_out.index(VOCAB+1)]]\n",
    "    else:\n",
    "        ref = [dec_out]\n",
    "    \n",
    "    bleu = BLEU(ref, cnd)\n",
    "    \n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "LR = 0.0002\n",
    "DECAY = 0.97\n",
    "\n",
    "loss_seq2seq = nn.CrossEntropyLoss(reduction='none').cuda()\n",
    "optim = T.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = []\n",
    "\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "    ls_ep = 0\n",
    "    \n",
    "    model.train()\n",
    "    with tqdm(ld_tr) as TQ:\n",
    "        for inp, dec_inp, dec_out, wgt in TQ:\n",
    "            out = model(inp.long().cuda(), is_tr=True, dec_inp=dec_inp.long().cuda())\n",
    "            \n",
    "            out = out.view((out.shape[0]*out.shape[1], out.shape[2]))\n",
    "            dec_out = dec_out.view((dec_out.shape[0]*dec_out.shape[1], ))\n",
    "            wgt = wgt.view((wgt.shape[0]*wgt.shape[1], )).float().cuda()\n",
    "            \n",
    "            ls_bh = loss_seq2seq(out, dec_out.long().cuda())\n",
    "            ls_bh = (ls_bh*wgt).sum() / wgt.sum()\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            ls_ep += ls_bh\n",
    "            \n",
    "            TQ.set_postfix(ls_bh='%.3f'%(ls_bh))\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: %.4f' % (e+1, ls_ep))\n",
    "    \n",
    "    for pg in optim.param_groups:\n",
    "        pg['lr'] *= DECAY\n",
    "    \n",
    "    T.save(model.state_dict(), 'Model/model_seq2seq_%d.pt'%(e+1))\n",
    "    \n",
    "    bleu_ep = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with tqdm(ld_vl) as TQ:\n",
    "        for inp, _, dec_out, _ in TQ:\n",
    "            out = model(inp.long().cuda())\n",
    "            \n",
    "            out = np.argmax(out.detach().cpu().numpy(), axis=2)\n",
    "            dec_out = dec_out.numpy()\n",
    "            \n",
    "            bleus = []\n",
    "            for i in range(out.shape[0]):\n",
    "                bleu = get_bleu(out[i], dec_out[i])\n",
    "                \n",
    "                bleus.append(bleu)\n",
    "            \n",
    "            bleu_bh = np.average(bleus)\n",
    "            bleu_ep += bleu_bh\n",
    "            \n",
    "            TQ.set_postfix(bleu_bh='%.3f%%'%(bleu_bh*100))\n",
    "        \n",
    "        bleu_ep /= len(TQ)\n",
    "        print('Valid: %.4f%%' % (bleu_ep*100))\n",
    "    \n",
    "    rec.append([ls_ep, bleu_ep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([v[0] for v in rec], label='loss_train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([v[1]*100 for v in rec], label='bleu_valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(T.load('Model/model_seq2seq_%d.pt'%(40)))\n",
    "\n",
    "bleu_ep = 0\n",
    "    \n",
    "model.eval()\n",
    "with tqdm(ld_ts) as TQ:\n",
    "    for inp, _, dec_out, _ in TQ:\n",
    "        out = model(inp.long().cuda())\n",
    "            \n",
    "        out = np.argmax(out.detach().cpu().numpy(), axis=2)\n",
    "        dec_out = dec_out.numpy()\n",
    "            \n",
    "        bleus = []\n",
    "        for i in range(out.shape[0]):\n",
    "            bleu = get_bleu(out[i], dec_out[i])\n",
    "                \n",
    "            bleus.append(bleu)\n",
    "            \n",
    "        bleu_bh = np.average(bleus)\n",
    "        bleu_ep += bleu_bh\n",
    "            \n",
    "        TQ.set_postfix(bleu_bh='%.3f%%'%(bleu_bh*100))\n",
    "        \n",
    "    bleu_ep /= len(TQ)\n",
    "    print('Test: %.4f%%' % (bleu_ep*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
